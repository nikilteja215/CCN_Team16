{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8157708,"sourceType":"datasetVersion","datasetId":4825889},{"sourceId":8157948,"sourceType":"datasetVersion","datasetId":4826077},{"sourceId":8159217,"sourceType":"datasetVersion","datasetId":4827022}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Collab preliminaries","metadata":{"id":"vSQFs_G8caeE"}},{"cell_type":"code","source":"# This code utilizes the NVIDIA CUDA Compiler (nvcc) to compile CUDA programs\n# for parallel computing on NVIDIA GPUs. Make sure you have the CUDA Toolkit\n# installed in your environment to use CUDA-related functionalities.\n!nvcc --version","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIVB0Xn1g6ih","outputId":"ba77d541-b7ae-4b49-e26a-2e81c6f43d21","execution":{"iopub.status.busy":"2024-04-18T18:17:59.904731Z","iopub.execute_input":"2024-04-18T18:17:59.905565Z","iopub.status.idle":"2024-04-18T18:18:00.866976Z","shell.execute_reply.started":"2024-04-18T18:17:59.905532Z","shell.execute_reply":"2024-04-18T18:18:00.865872Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp -ri \"/kaggle/input/wave2lip-utils\" /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:18:03.705291Z","iopub.execute_input":"2024-04-18T18:18:03.706131Z","iopub.status.idle":"2024-04-18T18:18:09.709050Z","shell.execute_reply.started":"2024-04-18T18:18:03.706086Z","shell.execute_reply":"2024-04-18T18:18:09.707776Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # Mounts Google Drive to access files and directories stored on your Google Drive\n# from google.colab import drive\n# drive.mount('/content/gdrive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qciH4PsUazL_","outputId":"a3803755-e369-408c-b328-308330e2de60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/gdrive\n"}]},{"cell_type":"markdown","source":"# Get the code and models","metadata":{"id":"yJ5taGmPcWV-"}},{"cell_type":"code","source":"# Clones the Wav2Lip GitHub repository to access the code and resources for running the Wav2Lip model\n!git clone https://github.com/Rudrabha/Wav2Lip.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3LihClHbUd3","outputId":"2a067f34-000f-4cf0-bc06-d174631fe737","execution":{"iopub.status.busy":"2024-04-18T18:18:09.824749Z","iopub.execute_input":"2024-04-18T18:18:09.825124Z","iopub.status.idle":"2024-04-18T18:18:11.949515Z","shell.execute_reply.started":"2024-04-18T18:18:09.825092Z","shell.execute_reply":"2024-04-18T18:18:11.948383Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'Wav2Lip'...\nremote: Enumerating objects: 381, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 381 (delta 0), reused 1 (delta 0), pack-reused 378\u001b[K\nReceiving objects: 100% (381/381), 534.01 KiB | 12.42 MiB/s, done.\nResolving deltas: 100% (210/210), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Lists the contents of the 'Wav2Lip' directory from Google Drive\n!ls /kaggle/working/wave2lip-utils/Wave2Lip_Videos","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-19nzx8SamJ","outputId":"9aeb1f20-9189-44be-dbda-f169dc9a2006","execution":{"iopub.status.busy":"2024-04-18T18:18:15.710492Z","iopub.execute_input":"2024-04-18T18:18:15.710916Z","iopub.status.idle":"2024-04-18T18:18:16.650322Z","shell.execute_reply.started":"2024-04-18T18:18:15.710878Z","shell.execute_reply":"2024-04-18T18:18:16.649077Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"kennedy.mp4\t kennedy1min.wav  mona.jpg  mona1min.mp4\nkennedy1min.mp4  korean.wav\t  mona.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:18:20.065068Z","iopub.execute_input":"2024-04-18T18:18:20.065439Z","iopub.status.idle":"2024-04-18T18:18:20.071977Z","shell.execute_reply.started":"2024-04-18T18:18:20.065409Z","shell.execute_reply":"2024-04-18T18:18:20.071058Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:18:22.184999Z","iopub.execute_input":"2024-04-18T18:18:22.185660Z","iopub.status.idle":"2024-04-18T18:18:23.116177Z","shell.execute_reply.started":"2024-04-18T18:18:22.185627Z","shell.execute_reply":"2024-04-18T18:18:23.115044Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Wav2Lip  wave2lip-utils\n","output_type":"stream"}]},{"cell_type":"code","source":"# Copies the pre-trained model file 'wav2lip_gan.pth' from Google Drive to the 'checkpoints' directory\n!cp -ri \"/kaggle/working/wave2lip-utils/Wave2Lip_pretrained/Wave2Lip_pretrained/wav2lip_gan.pth\" /kaggle/working/Wav2Lip/checkpoints","metadata":{"id":"YjzMPy_Sb0AI","execution":{"iopub.status.busy":"2024-04-18T18:18:27.395191Z","iopub.execute_input":"2024-04-18T18:18:27.395969Z","iopub.status.idle":"2024-04-18T18:18:28.679297Z","shell.execute_reply.started":"2024-04-18T18:18:27.395933Z","shell.execute_reply":"2024-04-18T18:18:28.678097Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Get the pre-requisites","metadata":{"id":"aWTaOS3ncFt6"}},{"cell_type":"code","source":"# Uninstalls TensorFlow and TensorFlow-GPU packages\n# Useful for updating or reinstalling TensorFlow dependencies in the Colab environment\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-gpu -y","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ooh28vw-Uvd3","outputId":"dc01eea2-da7f-4beb-c4a0-88fdf6cef72d","execution":{"iopub.status.busy":"2024-04-18T18:18:42.951865Z","iopub.execute_input":"2024-04-18T18:18:42.952243Z","iopub.status.idle":"2024-04-18T18:19:03.057975Z","shell.execute_reply.started":"2024-04-18T18:18:42.952209Z","shell.execute_reply":"2024-04-18T18:19:03.056757Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Found existing installation: tensorflow 2.15.0\nUninstalling tensorflow-2.15.0:\n  Successfully uninstalled tensorflow-2.15.0\n\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Changes directory to 'Wav2Lip' and installs the required Python packages listed in 'requirements.txt'\n# Ensures all necessary dependencies are installed for running the Wav2Lip model\n!cd Wav2Lip && pip install -r requirements.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49dCYlLdcK2D","outputId":"d892b00e-6da8-41c7-da95-6a91f0f6d5f0","execution":{"iopub.status.busy":"2024-04-18T18:19:06.916743Z","iopub.execute_input":"2024-04-18T18:19:06.917661Z","iopub.status.idle":"2024-04-18T18:19:11.961180Z","shell.execute_reply.started":"2024-04-18T18:19:06.917622Z","shell.execute_reply":"2024-04-18T18:19:11.960045Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting librosa==0.7.0 (from -r requirements.txt (line 1))\n  Downloading librosa-0.7.0.tar.gz (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting numpy==1.17.1 (from -r requirements.txt (line 2))\n  Downloading numpy-1.17.1.zip (6.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: opencv-contrib-python>=4.2.0.34 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.9.0.80)\n\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.1.0.25 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.1.0.25\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:22:42.948797Z","iopub.execute_input":"2024-04-18T18:22:42.949183Z","iopub.status.idle":"2024-04-18T18:22:55.336123Z","shell.execute_reply.started":"2024-04-18T18:22:42.949150Z","shell.execute_reply":"2024-04-18T18:22:55.335141Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Downloads the pre-trained face detection model 's3fd.pth' from Adrian Bulat's website\n# This model is used for face detection in the Wav2Lip project\n!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"Wav2Lip/face_detection/detection/sfd/s3fd.pth\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ey_bN4M6X_95","outputId":"8dece9aa-86bc-4d0a-d1b0-c8f0ac973818","execution":{"iopub.status.busy":"2024-04-18T18:22:56.343643Z","iopub.execute_input":"2024-04-18T18:22:56.344337Z","iopub.status.idle":"2024-04-18T18:22:57.820002Z","shell.execute_reply.started":"2024-04-18T18:22:56.344302Z","shell.execute_reply":"2024-04-18T18:22:57.818868Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"--2024-04-18 18:22:57--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\nResolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\nConnecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 89843225 (86M) [application/octet-stream]\nSaving to: 'Wav2Lip/face_detection/detection/sfd/s3fd.pth'\n\nWav2Lip/face_detect 100%[===================>]  85.68M   217MB/s    in 0.4s    \n\n2024-04-18 18:22:57 (217 MB/s) - 'Wav2Lip/face_detection/detection/sfd/s3fd.pth' saved [89843225/89843225]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Now lets try!","metadata":{"id":"qdIQfY2Kswcb"}},{"cell_type":"code","source":"import os\nos.mkdir(\"sample_data\")\n!cp \"/kaggle/working/wave2lip-utils/Wave2Lip_Videos/kennedy1min.wav\" sample_data/\n!ls sample_data/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoVGMtjRZfeR","outputId":"ba2039f2-1cab-4699-8695-45e854015017","execution":{"iopub.status.busy":"2024-04-18T18:23:08.523056Z","iopub.execute_input":"2024-04-18T18:23:08.523435Z","iopub.status.idle":"2024-04-18T18:23:10.406722Z","shell.execute_reply.started":"2024-04-18T18:23:08.523402Z","shell.execute_reply":"2024-04-18T18:23:10.405755Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"kennedy1min.wav\n","output_type":"stream"}]},{"cell_type":"code","source":"os.remove(\"/kaggle/working/Wav2Lip/audio.py\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:24:00.196326Z","iopub.execute_input":"2024-04-18T18:24:00.196723Z","iopub.status.idle":"2024-04-18T18:24:00.201791Z","shell.execute_reply.started":"2024-04-18T18:24:00.196688Z","shell.execute_reply":"2024-04-18T18:24:00.200859Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!cp \"/kaggle/working/wave2lip-utils/Wave2Lip_Videos/mona.mp4\" /kaggle/working/sample_data/\n!ls /kaggle/working/sample_data","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:26:05.953269Z","iopub.execute_input":"2024-04-18T18:26:05.953989Z","iopub.status.idle":"2024-04-18T18:26:07.826522Z","shell.execute_reply.started":"2024-04-18T18:26:05.953952Z","shell.execute_reply":"2024-04-18T18:26:07.825361Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"kennedy1min.wav  mona.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp \"/kaggle/input/audio/audio.py\" /kaggle/working/Wav2Lip/","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:24:28.118194Z","iopub.execute_input":"2024-04-18T18:24:28.118554Z","iopub.status.idle":"2024-04-18T18:24:29.066066Z","shell.execute_reply.started":"2024-04-18T18:24:28.118523Z","shell.execute_reply":"2024-04-18T18:24:29.064886Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/Wav2Lip/","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:24:39.423202Z","iopub.execute_input":"2024-04-18T18:24:39.423577Z","iopub.status.idle":"2024-04-18T18:24:40.358659Z","shell.execute_reply.started":"2024-04-18T18:24:39.423545Z","shell.execute_reply":"2024-04-18T18:24:40.357555Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"README.md\t\tface_detection\t     models\t       temp\naudio.py\t\tfilelists\t     mona.mp4\t       wav2lip_train.py\ncheckpoints\t\thparams.py\t     preprocess.py\ncolor_syncnet_train.py\thq_wav2lip_train.py  requirements.txt\nevaluation\t\tinference.py\t     results\n","output_type":"stream"}]},{"cell_type":"code","source":"# os.remove(\"/kaggle/working/Wav2Lip/audio.py\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T16:43:59.033172Z","iopub.execute_input":"2024-04-18T16:43:59.033555Z","iopub.status.idle":"2024-04-18T16:43:59.038464Z","shell.execute_reply.started":"2024-04-18T16:43:59.033525Z","shell.execute_reply":"2024-04-18T16:43:59.037319Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/mona.mp4\" --audio \"../sample_data/kennedy1min.wav\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jR5utmDMcSZY","outputId":"b03620ce-ea68-4cf1-f998-3ca4ff68c364","execution":{"iopub.status.busy":"2024-04-18T18:26:16.353296Z","iopub.execute_input":"2024-04-18T18:26:16.353991Z","iopub.status.idle":"2024-04-18T18:29:19.800998Z","shell.execute_reply.started":"2024-04-18T18:26:16.353957Z","shell.execute_reply":"2024-04-18T18:29:19.799787Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Using cuda for inference.\nReading video frames...\nNumber of frames available for inference: 479\n(80, 6402)\nLength of mel chunks: 2394\n  0%|                                                    | 0/19 [00:00<?, ?it/s]\n  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/30 [00:19<09:31, 19.69s/it]\u001b[A\n  7%|██▉                                         | 2/30 [00:21<04:17,  9.19s/it]\u001b[A\n 10%|████▍                                       | 3/30 [00:23<02:35,  5.77s/it]\u001b[A\n 13%|█████▊                                      | 4/30 [00:24<01:48,  4.17s/it]\u001b[A\n 17%|███████▎                                    | 5/30 [00:26<01:22,  3.28s/it]\u001b[A\n 20%|████████▊                                   | 6/30 [00:28<01:06,  2.75s/it]\u001b[A\n 23%|██████████▎                                 | 7/30 [00:30<00:55,  2.41s/it]\u001b[A\n 27%|███████████▋                                | 8/30 [00:31<00:48,  2.19s/it]\u001b[A\n 30%|█████████████▏                              | 9/30 [00:33<00:43,  2.06s/it]\u001b[A\n 33%|██████████████▎                            | 10/30 [00:35<00:39,  1.95s/it]\u001b[A\n 37%|███████████████▊                           | 11/30 [00:36<00:35,  1.88s/it]\u001b[A\n 40%|█████████████████▏                         | 12/30 [00:38<00:32,  1.83s/it]\u001b[A\n 43%|██████████████████▋                        | 13/30 [00:40<00:30,  1.80s/it]\u001b[A\n 47%|████████████████████                       | 14/30 [00:42<00:28,  1.77s/it]\u001b[A\n 50%|█████████████████████▌                     | 15/30 [00:43<00:26,  1.75s/it]\u001b[A\n 53%|██████████████████████▉                    | 16/30 [00:45<00:24,  1.73s/it]\u001b[A\n 57%|████████████████████████▎                  | 17/30 [00:47<00:22,  1.73s/it]\u001b[A\n 60%|█████████████████████████▊                 | 18/30 [00:48<00:20,  1.73s/it]\u001b[A\n 63%|███████████████████████████▏               | 19/30 [00:50<00:18,  1.72s/it]\u001b[A\n 67%|████████████████████████████▋              | 20/30 [00:52<00:17,  1.72s/it]\u001b[A\n 70%|██████████████████████████████             | 21/30 [00:54<00:15,  1.72s/it]\u001b[A\n 73%|███████████████████████████████▌           | 22/30 [00:55<00:13,  1.71s/it]\u001b[A\n 77%|████████████████████████████████▉          | 23/30 [00:57<00:11,  1.71s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 24/30 [00:59<00:10,  1.71s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 25/30 [01:00<00:08,  1.71s/it]\u001b[A\n 87%|█████████████████████████████████████▎     | 26/30 [01:02<00:06,  1.71s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 27/30 [01:04<00:05,  1.73s/it]\u001b[A\n 93%|████████████████████████████████████████▏  | 28/30 [01:06<00:03,  1.72s/it]\u001b[A\n 97%|█████████████████████████████████████████▌ | 29/30 [01:07<00:01,  1.73s/it]\u001b[A\n100%|███████████████████████████████████████████| 30/30 [01:33<00:00,  3.12s/it]\u001b[A\nLoad checkpoint from: checkpoints/wav2lip_gan.pth\nModel loaded\n100%|███████████████████████████████████████████| 19/19 [02:24<00:00,  7.63s/it]\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n\u001b[0mInput #0, wav, from '../sample_data/kennedy1min.wav':\n  Metadata:\n    encoder         : Lavf58.76.100\n  Duration: 00:01:20.02, bitrate: 1536 kb/s\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\nInput #1, avi, from 'temp/result.avi':\n  Metadata:\n    encoder         : Lavf59.27.100\n  Duration: 00:01:19.88, start: 0.000000, bitrate: 2108 kb/s\n    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 2102 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 2997 tbc\nStream mapping:\n  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\nPress [q] to stop, [?] for help\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n\u001b[0m\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0musing SAR=1/1\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mprofile High, level 3.1\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'results/result_voice.mp4':\n  Metadata:\n    encoder         : Lavf58.29.100\n    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 29.97 fps, 30k tbn, 29.97 tbc\n    Metadata:\n      encoder         : Lavc58.54.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n    Metadata:\n      encoder         : Lavc58.54.100 aac\nframe= 2394 fps= 99 q=-1.0 Lsize=    4178kB time=00:01:20.02 bitrate= 427.7kbits/s speed=3.31x    \nvideo:2835kB audio:1256kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.130449%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mframe I:10    Avg QP:15.48  size:118783\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mframe P:624   Avg QP:18.95  size:  2043\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mframe B:1760  Avg QP:30.40  size:   250\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mconsecutive B-frames:  1.0%  2.8%  0.9% 95.4%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mmb I  I16..4:  1.4% 97.6%  1.0%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mmb P  I16..4:  0.1%  0.5%  0.0%  P16..4:  7.7%  1.3%  1.3%  0.0%  0.0%    skip:89.1%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  4.4%  0.1%  0.0%  direct: 0.0%  skip:95.5%  L0:46.1% L1:53.0% BI: 1.0%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0m8x8 transform intra:93.9% inter:93.5%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mcoded y,uvDC,uvAC intra: 90.7% 85.8% 37.4% inter: 1.1% 1.4% 0.1%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mi16 v,h,dc,p: 16% 21%  2% 61%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 18% 27%  4%  4%  5%  4%  7%  7%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 38% 17% 11%  5%  8%  8%  5%  5%  3%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mi8c dc,h,v,p: 40% 23% 26% 12%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mref P L0: 69.3%  6.6% 15.1%  9.1%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mref B L0: 84.5% 12.1%  3.4%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mref B L1: 91.4%  8.6%\n\u001b[1;36m[libx264 @ 0x57fb5a913740] \u001b[0mkb/s:290.66\n\u001b[1;36m[aac @ 0x57fb5a90f9c0] \u001b[0mQavg: 283.455\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# use the \"files\" button on the left to download the result in the Wav2Lip/results/ folder.","metadata":{"id":"uNOAZvkszEOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Variations to try**\n","metadata":{"id":"d7zgfrQqbKom"}},{"cell_type":"markdown","source":"1.   Use more padding to include the chin region","metadata":{"id":"0f9A9VDVbZAG"}},{"cell_type":"code","source":"!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/mona.mp4\" --audio \"../sample_data/kennedy1min.wav\" --pads 0 20 0 0 --resize_factor 2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45XW4SZAzIz5","outputId":"db8328a6-8aa8-42db-8830-04942764fbf5","execution":{"iopub.status.busy":"2024-04-18T18:29:19.802976Z","iopub.execute_input":"2024-04-18T18:29:19.803289Z","iopub.status.idle":"2024-04-18T18:30:40.364632Z","shell.execute_reply.started":"2024-04-18T18:29:19.803259Z","shell.execute_reply":"2024-04-18T18:30:40.363449Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Using cuda for inference.\nReading video frames...\nNumber of frames available for inference: 479\n(80, 6402)\nLength of mel chunks: 2394\n  0%|                                                    | 0/19 [00:00<?, ?it/s]\n  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/30 [00:05<02:37,  5.42s/it]\u001b[A\n  7%|██▉                                         | 2/30 [00:05<01:10,  2.52s/it]\u001b[A\n 10%|████▍                                       | 3/30 [00:06<00:43,  1.60s/it]\u001b[A\n 13%|█████▊                                      | 4/30 [00:06<00:30,  1.17s/it]\u001b[A\n 17%|███████▎                                    | 5/30 [00:07<00:23,  1.08it/s]\u001b[A\n 20%|████████▊                                   | 6/30 [00:07<00:18,  1.28it/s]\u001b[A\n 23%|██████████▎                                 | 7/30 [00:08<00:15,  1.46it/s]\u001b[A\n 27%|███████████▋                                | 8/30 [00:08<00:13,  1.60it/s]\u001b[A\n 30%|█████████████▏                              | 9/30 [00:09<00:12,  1.71it/s]\u001b[A\n 33%|██████████████▎                            | 10/30 [00:09<00:11,  1.80it/s]\u001b[A\n 37%|███████████████▊                           | 11/30 [00:10<00:10,  1.86it/s]\u001b[A\n 40%|█████████████████▏                         | 12/30 [00:10<00:09,  1.91it/s]\u001b[A\n 43%|██████████████████▋                        | 13/30 [00:11<00:08,  1.93it/s]\u001b[A\n 47%|████████████████████                       | 14/30 [00:11<00:08,  1.95it/s]\u001b[A\n 50%|█████████████████████▌                     | 15/30 [00:12<00:07,  1.97it/s]\u001b[A\n 53%|██████████████████████▉                    | 16/30 [00:12<00:07,  1.93it/s]\u001b[A\n 57%|████████████████████████▎                  | 17/30 [00:13<00:06,  1.95it/s]\u001b[A\n 60%|█████████████████████████▊                 | 18/30 [00:13<00:06,  1.97it/s]\u001b[A\n 63%|███████████████████████████▏               | 19/30 [00:14<00:05,  1.98it/s]\u001b[A\n 67%|████████████████████████████▋              | 20/30 [00:14<00:05,  1.99it/s]\u001b[A\n 70%|██████████████████████████████             | 21/30 [00:15<00:04,  2.00it/s]\u001b[A\n 73%|███████████████████████████████▌           | 22/30 [00:15<00:04,  2.00it/s]\u001b[A\n 77%|████████████████████████████████▉          | 23/30 [00:16<00:03,  2.00it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 24/30 [00:16<00:03,  1.98it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 25/30 [00:17<00:02,  1.99it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 26/30 [00:17<00:02,  1.99it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 27/30 [00:18<00:01,  2.00it/s]\u001b[A\n 93%|████████████████████████████████████████▏  | 28/30 [00:18<00:00,  2.00it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 29/30 [00:19<00:00,  2.00it/s]\u001b[A\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.10it/s]\u001b[A\nLoad checkpoint from: checkpoints/wav2lip_gan.pth\nModel loaded\n100%|███████████████████████████████████████████| 19/19 [01:06<00:00,  3.52s/it]\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n\u001b[0mInput #0, wav, from '../sample_data/kennedy1min.wav':\n  Metadata:\n    encoder         : Lavf58.76.100\n  Duration: 00:01:20.02, bitrate: 1536 kb/s\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\nInput #1, avi, from 'temp/result.avi':\n  Metadata:\n    encoder         : Lavf59.27.100\n  Duration: 00:01:19.88, start: 0.000000, bitrate: 767 kb/s\n    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 761 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 2997 tbc\nStream mapping:\n  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\nPress [q] to stop, [?] for help\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n\u001b[0m\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0musing SAR=1/1\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mprofile High, level 3.0\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'results/result_voice.mp4':\n  Metadata:\n    encoder         : Lavf58.29.100\n    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 640x360 [SAR 1:1 DAR 16:9], q=-1--1, 29.97 fps, 30k tbn, 29.97 tbc\n    Metadata:\n      encoder         : Lavc58.54.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n    Metadata:\n      encoder         : Lavc58.54.100 aac\nframe= 2394 fps=371 q=-1.0 Lsize=    2231kB time=00:01:20.02 bitrate= 228.4kbits/s speed=12.4x    \nvideo:889kB audio:1256kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 4.044037%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mframe I:10    Avg QP:16.04  size: 40077\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mframe P:735   Avg QP:20.37  size:   530\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mframe B:1649  Avg QP:31.37  size:    72\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mconsecutive B-frames:  4.9%  8.4%  4.1% 82.5%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mmb I  I16..4:  0.4% 99.1%  0.5%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mmb P  I16..4:  0.0%  0.1%  0.0%  P16..4:  6.7%  1.5%  1.2%  0.0%  0.0%    skip:90.5%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  3.5%  0.1%  0.0%  direct: 0.0%  skip:96.3%  L0:47.4% L1:51.0% BI: 1.6%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0m8x8 transform intra:98.7% inter:91.4%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mcoded y,uvDC,uvAC intra: 99.1% 92.5% 61.2% inter: 1.2% 1.5% 0.1%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mi16 v,h,dc,p:  9%  0%  0% 91%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 16% 29%  5%  4%  5%  4%  7%  8%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 24%  7%  6%  7% 10%  8%  4%  6%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mi8c dc,h,v,p: 42% 24% 24% 11%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mref P L0: 69.0% 10.3% 12.8%  7.9%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mref B L0: 83.4% 13.2%  3.4%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mref B L1: 92.6%  7.4%\n\u001b[1;36m[libx264 @ 0x5b08ab7ae840] \u001b[0mkb/s:91.08\n\u001b[1;36m[aac @ 0x5b08ab7a9640] \u001b[0mQavg: 283.455\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"2.   Use resize_factor to reduce the video resolution, as there is a change you might get better results for lower resolution videos. Why? Because the model was trained on low resolution faces.","metadata":{"id":"uo-WnsxfbwTG"}},{"cell_type":"code","source":"!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../sample_data/mona.mp4\" --audio \"../sample_data/kennedy1min.wav\" --resize_factor 2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw0xFtZ2bsx8","outputId":"ba317057-1b37-470f-e5aa-2b6bdb76ed6a","execution":{"iopub.status.busy":"2024-04-18T18:30:40.366232Z","iopub.execute_input":"2024-04-18T18:30:40.366618Z","iopub.status.idle":"2024-04-18T18:32:00.734590Z","shell.execute_reply.started":"2024-04-18T18:30:40.366579Z","shell.execute_reply":"2024-04-18T18:32:00.733620Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Using cuda for inference.\nReading video frames...\nNumber of frames available for inference: 479\n(80, 6402)\nLength of mel chunks: 2394\n  0%|                                                    | 0/19 [00:00<?, ?it/s]\n  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 1/30 [00:05<02:37,  5.44s/it]\u001b[A\n  7%|██▉                                         | 2/30 [00:05<01:10,  2.53s/it]\u001b[A\n 10%|████▍                                       | 3/30 [00:06<00:43,  1.61s/it]\u001b[A\n 13%|█████▊                                      | 4/30 [00:06<00:30,  1.17s/it]\u001b[A\n 17%|███████▎                                    | 5/30 [00:07<00:23,  1.08it/s]\u001b[A\n 20%|████████▊                                   | 6/30 [00:07<00:18,  1.29it/s]\u001b[A\n 23%|██████████▎                                 | 7/30 [00:08<00:15,  1.46it/s]\u001b[A\n 27%|███████████▋                                | 8/30 [00:08<00:13,  1.61it/s]\u001b[A\n 30%|█████████████▏                              | 9/30 [00:09<00:12,  1.73it/s]\u001b[A\n 33%|██████████████▎                            | 10/30 [00:09<00:10,  1.82it/s]\u001b[A\n 37%|███████████████▊                           | 11/30 [00:10<00:10,  1.89it/s]\u001b[A\n 40%|█████████████████▏                         | 12/30 [00:10<00:09,  1.94it/s]\u001b[A\n 43%|██████████████████▋                        | 13/30 [00:11<00:08,  1.98it/s]\u001b[A\n 47%|████████████████████                       | 14/30 [00:11<00:07,  2.01it/s]\u001b[A\n 50%|█████████████████████▌                     | 15/30 [00:12<00:07,  2.03it/s]\u001b[A\n 53%|██████████████████████▉                    | 16/30 [00:12<00:06,  2.03it/s]\u001b[A\n 57%|████████████████████████▎                  | 17/30 [00:13<00:06,  2.04it/s]\u001b[A\n 60%|█████████████████████████▊                 | 18/30 [00:13<00:05,  2.04it/s]\u001b[A\n 63%|███████████████████████████▏               | 19/30 [00:14<00:05,  2.05it/s]\u001b[A\n 67%|████████████████████████████▋              | 20/30 [00:14<00:04,  2.06it/s]\u001b[A\n 70%|██████████████████████████████             | 21/30 [00:15<00:04,  2.06it/s]\u001b[A\n 73%|███████████████████████████████▌           | 22/30 [00:15<00:03,  2.06it/s]\u001b[A\n 77%|████████████████████████████████▉          | 23/30 [00:16<00:03,  2.04it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 24/30 [00:16<00:02,  2.03it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 25/30 [00:17<00:02,  2.04it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 26/30 [00:17<00:01,  2.04it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 27/30 [00:18<00:01,  2.04it/s]\u001b[A\n 93%|████████████████████████████████████████▏  | 28/30 [00:18<00:00,  2.04it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 29/30 [00:19<00:00,  2.04it/s]\u001b[A\n100%|███████████████████████████████████████████| 30/30 [00:26<00:00,  1.12it/s]\u001b[A\nLoad checkpoint from: checkpoints/wav2lip_gan.pth\nModel loaded\n100%|███████████████████████████████████████████| 19/19 [01:06<00:00,  3.51s/it]\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n\u001b[0mInput #0, wav, from '../sample_data/kennedy1min.wav':\n  Metadata:\n    encoder         : Lavf58.76.100\n  Duration: 00:01:20.02, bitrate: 1536 kb/s\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\nInput #1, avi, from 'temp/result.avi':\n  Metadata:\n    encoder         : Lavf59.27.100\n  Duration: 00:01:19.88, start: 0.000000, bitrate: 770 kb/s\n    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 764 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 2997 tbc\nStream mapping:\n  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\nPress [q] to stop, [?] for help\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n\u001b[0m\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0musing SAR=1/1\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mprofile High, level 3.0\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\nOutput #0, mp4, to 'results/result_voice.mp4':\n  Metadata:\n    encoder         : Lavf58.29.100\n    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 640x360 [SAR 1:1 DAR 16:9], q=-1--1, 29.97 fps, 30k tbn, 29.97 tbc\n    Metadata:\n      encoder         : Lavc58.54.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n    Metadata:\n      encoder         : Lavc58.54.100 aac\nframe= 2394 fps=365 q=-1.0 Lsize=    2227kB time=00:01:20.02 bitrate= 227.9kbits/s speed=12.2x    \nvideo:884kB audio:1256kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 4.054954%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mframe I:10    Avg QP:16.03  size: 40207\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mframe P:734   Avg QP:20.24  size:   526\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mframe B:1650  Avg QP:31.49  size:    70\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mconsecutive B-frames:  4.8%  8.9%  3.5% 82.9%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mmb I  I16..4:  0.4% 99.2%  0.4%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mmb P  I16..4:  0.0%  0.1%  0.0%  P16..4:  6.6%  1.5%  1.2%  0.0%  0.0%    skip:90.5%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  3.4%  0.2%  0.0%  direct: 0.0%  skip:96.4%  L0:43.2% L1:55.2% BI: 1.6%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0m8x8 transform intra:98.8% inter:92.1%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mcoded y,uvDC,uvAC intra: 99.1% 92.5% 61.4% inter: 1.2% 1.5% 0.1%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mi16 v,h,dc,p:  6%  2%  0% 92%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 16% 29%  4%  4%  5%  5%  7%  8%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 26%  7%  3%  8% 11%  9%  5%  6%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mi8c dc,h,v,p: 42% 24% 24% 10%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mref P L0: 69.2% 10.6% 12.4%  7.9%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mref B L0: 87.3%  9.5%  3.2%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mref B L1: 91.7%  8.3%\n\u001b[1;36m[libx264 @ 0x5d4afbf768c0] \u001b[0mkb/s:90.58\n\u001b[1;36m[aac @ 0x5d4afbf71680] \u001b[0mQavg: 283.455\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Let's adapt the inference code for Wav2Lip to export the PyTorch model and then convert it to Core ML:**bold text**","metadata":{"id":"eBQ6lnbTyGLB"}},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-04-18T16:54:03.218351Z","iopub.execute_input":"2024-04-18T16:54:03.218808Z","iopub.status.idle":"2024-04-18T16:55:08.396396Z","shell.execute_reply.started":"2024-04-18T16:54:03.218768Z","shell.execute_reply":"2024-04-18T16:55:08.395273Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/README.md (deflated 58%)\n  adding: kaggle/working/Wav2Lip/filelists/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/filelists/README.md (stored 0%)\n  adding: kaggle/working/Wav2Lip/wav2lip_train.py (deflated 72%)\n  adding: kaggle/working/Wav2Lip/.git/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/config (deflated 31%)\n  adding: kaggle/working/Wav2Lip/.git/objects/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/objects/pack/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/objects/pack/pack-180dd72707bcd892560f4775164c5d10c25b402d.idx (deflated 6%)\n  adding: kaggle/working/Wav2Lip/.git/objects/pack/pack-180dd72707bcd892560f4775164c5d10c25b402d.pack (deflated 1%)\n  adding: kaggle/working/Wav2Lip/.git/objects/info/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/description (deflated 14%)\n  adding: kaggle/working/Wav2Lip/.git/branches/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/info/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/info/exclude (deflated 28%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-receive.sample (deflated 40%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-push.sample (deflated 50%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/fsmonitor-watchman.sample (deflated 52%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/update.sample (deflated 68%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/commit-msg.sample (deflated 44%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/post-update.sample (deflated 27%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-merge-commit.sample (deflated 39%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-rebase.sample (deflated 59%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-commit.sample (deflated 45%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/pre-applypatch.sample (deflated 38%)\n  adding: kaggle/working/Wav2Lip/.git/hooks/applypatch-msg.sample (deflated 42%)\n  adding: kaggle/working/Wav2Lip/.git/index (deflated 52%)\n  adding: kaggle/working/Wav2Lip/.git/HEAD (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/HEAD (deflated 26%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/remotes/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/remotes/origin/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/heads/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/logs/refs/heads/master (deflated 26%)\n  adding: kaggle/working/Wav2Lip/.git/packed-refs (deflated 11%)\n  adding: kaggle/working/Wav2Lip/.git/refs/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/remotes/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/remotes/origin/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/remotes/origin/HEAD (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/tags/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/heads/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/.git/refs/heads/master (stored 0%)\n  adding: kaggle/working/Wav2Lip/evaluation/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/evaluation/README.md (deflated 60%)\n  adding: kaggle/working/Wav2Lip/evaluation/real_videos_inference.py (deflated 64%)\n  adding: kaggle/working/Wav2Lip/evaluation/gen_videos_from_filelist.py (deflated 64%)\n  adding: kaggle/working/Wav2Lip/evaluation/scores_LSE/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/evaluation/scores_LSE/calculate_scores_real_videos.sh (deflated 48%)\n  adding: kaggle/working/Wav2Lip/evaluation/scores_LSE/SyncNetInstance_calc_scores.py (deflated 70%)\n  adding: kaggle/working/Wav2Lip/evaluation/scores_LSE/calculate_scores_real_videos.py (deflated 59%)\n  adding: kaggle/working/Wav2Lip/evaluation/scores_LSE/calculate_scores_LRS.py (deflated 59%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/README.md (deflated 47%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/ReSyncED/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/ReSyncED/tts_pairs.txt (deflated 60%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/ReSyncED/random_pairs.txt (deflated 84%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/lrs3.txt (deflated 84%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/lrs2.txt (deflated 84%)\n  adding: kaggle/working/Wav2Lip/evaluation/test_filelists/lrw.txt (deflated 73%)\n  adding: kaggle/working/Wav2Lip/inference.py (deflated 63%)\n  adding: kaggle/working/Wav2Lip/audio.py (deflated 71%)\n  adding: kaggle/working/Wav2Lip/face_detection/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/README.md (deflated 29%)\n  adding: kaggle/working/Wav2Lip/face_detection/utils.py (deflated 70%)\n  adding: kaggle/working/Wav2Lip/face_detection/__init__.py (deflated 17%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/sfd_detector.py (deflated 63%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/s3fd.pth (deflated 84%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__init__.py (deflated 21%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/net_s3fd.py (deflated 82%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/net_s3fd.cpython-310.pyc (deflated 56%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/__init__.cpython-310.pyc (deflated 25%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/sfd_detector.cpython-310.pyc (deflated 50%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/bbox.cpython-310.pyc (deflated 53%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/__pycache__/detect.cpython-310.pyc (deflated 47%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/bbox.py (deflated 68%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/sfd/detect.py (deflated 72%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/__init__.py (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/__pycache__/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/__pycache__/__init__.cpython-310.pyc (deflated 22%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/__pycache__/core.cpython-310.pyc (deflated 54%)\n  adding: kaggle/working/Wav2Lip/face_detection/detection/core.py (deflated 68%)\n  adding: kaggle/working/Wav2Lip/face_detection/__pycache__/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/face_detection/__pycache__/models.cpython-310.pyc (deflated 52%)\n  adding: kaggle/working/Wav2Lip/face_detection/__pycache__/__init__.cpython-310.pyc (deflated 19%)\n  adding: kaggle/working/Wav2Lip/face_detection/__pycache__/utils.cpython-310.pyc (deflated 55%)\n  adding: kaggle/working/Wav2Lip/face_detection/__pycache__/api.cpython-310.pyc (deflated 43%)\n  adding: kaggle/working/Wav2Lip/face_detection/models.py (deflated 78%)\n  adding: kaggle/working/Wav2Lip/face_detection/api.py (deflated 58%)\n  adding: kaggle/working/Wav2Lip/color_syncnet_train.py (deflated 71%)\n  adding: kaggle/working/Wav2Lip/hq_wav2lip_train.py (deflated 76%)\n  adding: kaggle/working/Wav2Lip/results/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/results/README.md (deflated 2%)\n  adding: kaggle/working/Wav2Lip/results/result_voice.mp4 (deflated 7%)\n  adding: kaggle/working/Wav2Lip/requirements.txt (deflated 27%)\n  adding: kaggle/working/Wav2Lip/checkpoints/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/checkpoints/README.md (stored 0%)\n  adding: kaggle/working/Wav2Lip/checkpoints/wav2lip_gan.pth (deflated 9%)\n  adding: kaggle/working/Wav2Lip/__pycache__/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/__pycache__/hparams.cpython-310.pyc (deflated 38%)\n  adding: kaggle/working/Wav2Lip/__pycache__/audio.cpython-310.pyc (deflated 51%)\n  adding: kaggle/working/Wav2Lip/.gitignore (deflated 27%)\n  adding: kaggle/working/Wav2Lip/hparams.py (deflated 50%)\n  adding: kaggle/working/Wav2Lip/preprocess.py (deflated 56%)\n  adding: kaggle/working/Wav2Lip/models/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/models/__init__.py (deflated 20%)\n  adding: kaggle/working/Wav2Lip/models/syncnet.py (deflated 84%)\n  adding: kaggle/working/Wav2Lip/models/__pycache__/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/models/__pycache__/wav2lip.cpython-310.pyc (deflated 56%)\n  adding: kaggle/working/Wav2Lip/models/__pycache__/__init__.cpython-310.pyc (deflated 20%)\n  adding: kaggle/working/Wav2Lip/models/__pycache__/syncnet.cpython-310.pyc (deflated 52%)\n  adding: kaggle/working/Wav2Lip/models/__pycache__/conv.cpython-310.pyc (deflated 51%)\n  adding: kaggle/working/Wav2Lip/models/wav2lip.py (deflated 84%)\n  adding: kaggle/working/Wav2Lip/models/conv.py (deflated 78%)\n  adding: kaggle/working/Wav2Lip/temp/ (stored 0%)\n  adding: kaggle/working/Wav2Lip/temp/README.md (deflated 13%)\n  adding: kaggle/working/Wav2Lip/temp/result.avi (deflated 3%)\n  adding: kaggle/working/Wav2Lip/mona.mp4 (deflated 27%)\n  adding: kaggle/working/sample_data/ (stored 0%)\n  adding: kaggle/working/sample_data/kennedy1min.wav (deflated 48%)\n  adding: kaggle/working/sample_data/mona.mp4 (deflated 27%)\n  adding: kaggle/working/.ipynb_checkpoints/ (stored 0%)\n  adding: kaggle/working/wave2lip-utils/ (stored 0%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/ (stored 0%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/korean.wav (deflated 37%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/kennedy.mp4 (deflated 0%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/mona1min.mp4 (deflated 19%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/mona.jpg (deflated 1%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/kennedy1min.mp4 (deflated 2%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/kennedy1min.wav (deflated 48%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_Videos/mona.mp4 (deflated 27%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_pretrained/ (stored 0%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_pretrained/Wave2Lip_pretrained/ (stored 0%)\n  adding: kaggle/working/wave2lip-utils/Wave2Lip_pretrained/Wave2Lip_pretrained/wav2lip_gan.pth (deflated 9%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T16:55:08.399140Z","iopub.execute_input":"2024-04-18T16:55:08.400039Z","iopub.status.idle":"2024-04-18T16:55:08.408150Z","shell.execute_reply.started":"2024-04-18T16:55:08.399994Z","shell.execute_reply":"2024-04-18T16:55:08.407192Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install coremltools","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:32:33.091247Z","iopub.execute_input":"2024-04-18T18:32:33.091609Z","iopub.status.idle":"2024-04-18T18:32:47.523999Z","shell.execute_reply.started":"2024-04-18T18:32:33.091580Z","shell.execute_reply":"2024-04-18T18:32:47.522743Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Collecting coremltools\n  Downloading coremltools-7.1-cp310-none-manylinux1_x86_64.whl.metadata (2.4 kB)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from coremltools) (1.26.4)\nRequirement already satisfied: protobuf<=4.0.0,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from coremltools) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from coremltools) (1.12)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from coremltools) (4.66.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from coremltools) (21.3)\nRequirement already satisfied: attrs>=21.3.0 in /opt/conda/lib/python3.10/site-packages (from coremltools) (23.2.0)\nCollecting cattrs (from coremltools)\n  Downloading cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pyaml in /opt/conda/lib/python3.10/site-packages (from coremltools) (23.12.0)\nRequirement already satisfied: exceptiongroup>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from cattrs->coremltools) (1.2.0)\nRequirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from cattrs->coremltools) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->coremltools) (3.1.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from pyaml->coremltools) (6.0.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->coremltools) (1.3.0)\nDownloading coremltools-7.1-cp310-none-manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: cattrs, coremltools\nSuccessfully installed cattrs-23.2.3 coremltools-7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd /kaggle/working/Wav2Lip/Wave2Lip","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:33:57.433139Z","iopub.execute_input":"2024-04-18T18:33:57.433849Z","iopub.status.idle":"2024-04-18T18:33:58.409543Z","shell.execute_reply.started":"2024-04-18T18:33:57.433790Z","shell.execute_reply":"2024-04-18T18:33:58.408431Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/bin/bash: line 0: cd: /kaggle/working/Wav2Lip/Wave2Lip: No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd Wav2Lip","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:34:22.107832Z","iopub.execute_input":"2024-04-18T18:34:22.108488Z","iopub.status.idle":"2024-04-18T18:34:22.114188Z","shell.execute_reply.started":"2024-04-18T18:34:22.108456Z","shell.execute_reply":"2024-04-18T18:34:22.113314Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"/kaggle/working/Wav2Lip\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:34:28.668433Z","iopub.execute_input":"2024-04-18T18:34:28.669390Z","iopub.status.idle":"2024-04-18T18:34:29.629011Z","shell.execute_reply.started":"2024-04-18T18:34:28.669356Z","shell.execute_reply":"2024-04-18T18:34:29.627866Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"README.md\t\tevaluation\t     inference.py      results\n__pycache__\t\tface_detection\t     models\t       temp\naudio.py\t\tfilelists\t     mona.mp4\t       wav2lip_train.py\ncheckpoints\t\thparams.py\t     preprocess.py\ncolor_syncnet_train.py\thq_wav2lip_train.py  requirements.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:34:36.224059Z","iopub.execute_input":"2024-04-18T18:34:36.224465Z","iopub.status.idle":"2024-04-18T18:34:36.231449Z","shell.execute_reply.started":"2024-04-18T18:34:36.224432Z","shell.execute_reply":"2024-04-18T18:34:36.230617Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/working/Wav2Lip/checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:35:51.693095Z","iopub.execute_input":"2024-04-18T18:35:51.693863Z","iopub.status.idle":"2024-04-18T18:35:52.495524Z","shell.execute_reply.started":"2024-04-18T18:35:51.693826Z","shell.execute_reply":"2024-04-18T18:35:52.494604Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Wav2Lip(\n  (face_encoder_blocks): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(6, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (1): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (2): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (3): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (3): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (4): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (5): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (6): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n  )\n  (audio_encoder): Sequential(\n    (0): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (1): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (2): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (3): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(3, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (4): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (5): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (6): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (7): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (8): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (9): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(3, 2), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (10): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (11): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (12): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n  )\n  (face_decoder_blocks): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (1): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (2): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (3): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(768, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (4): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (5): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(320, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (6): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(160, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n  )\n  (output_block): Sequential(\n    (0): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (1): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n    (2): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Define example input shapes for face and audio (adjust according to your model's input shape)\nexample_face_input = torch.randn(1, 6, 96, 96)  # Example face input shape: [batch_size, num_channels, height, width]\nexample_audio_input = torch.randn(1, 1, 100, 40)  # Example audio input shape: [batch_size, num_channels, time_steps, features]\n\n# Convert to CoreML format\ncoreml_model = ct.convert(wav2lip_model, inputs=[\n    ct.TensorType(name=\"face_input\", shape=example_face_input.shape),\n    ct.TensorType(name=\"audio_input\", shape=example_audio_input.shape)\n])\n\n# Save the CoreML model to a file\ncoreml_model.save(\"converted_wav2lip_model.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:50:24.570191Z","iopub.execute_input":"2024-04-18T18:50:24.571019Z","iopub.status.idle":"2024-04-18T18:50:24.711583Z","shell.execute_reply.started":"2024-04-18T18:50:24.570980Z","shell.execute_reply":"2024-04-18T18:50:24.710386Z"},"trusted":true},"execution_count":47,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m example_audio_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m40\u001b[39m)  \u001b[38;5;66;03m# Example audio input shape: [batch_size, num_channels, time_steps, features]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert to CoreML format\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m coreml_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav2lip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mface_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_face_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_audio_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save the CoreML model to a file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m coreml_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted_wav2lip_model.mlmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/coremltools/converters/_converters_entry.py:527\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    525\u001b[0m _check_deployment_target(minimum_deployment_target)\n\u001b[1;32m    526\u001b[0m outputs_as_strings, outputs_as_tensor_or_image_types \u001b[38;5;241m=\u001b[39m _validate_outputs_argument(outputs)\n\u001b[0;32m--> 527\u001b[0m exact_source \u001b[38;5;241m=\u001b[39m \u001b[43m_determine_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs_as_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m source_dialect \u001b[38;5;241m=\u001b[39m _determine_source_dialect(model, exact_source)\n\u001b[1;32m    532\u001b[0m exact_target \u001b[38;5;241m=\u001b[39m _determine_target(convert_to, minimum_deployment_target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/coremltools/converters/_converters_entry.py:977\u001b[0m, in \u001b[0;36m_determine_source\u001b[0;34m(model, source, output_names, outputs_as_tensor_or_image_types, output_argument_as_specified_by_user)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    968\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to determine the type of the model, i.e. the source framework. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide the value of argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, from one of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have `tensorflow==1.14` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m )\n\u001b[0;32m--> 977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n","\u001b[0;31mValueError\u001b[0m: Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"milinternal\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you're converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed."],"ename":"ValueError","evalue":"Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"milinternal\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you're converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip  # Import the Wav2Lip model definition from the cloned repository\n\n# Load the Wav2Lip model checkpoint\ncheckpoint_path = \"checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()\n\n# Define example input shapes for face and audio (adjust according to your model's input shape)\nexample_face_input = torch.randn(1, 6, 96, 96)  # Example face input shape: [batch_size, num_channels, height, width]\nexample_audio_input = torch.randn(1, 1, 100, 40)  # Example audio input shape: [batch_size, num_channels, time_steps, features]\n\n# Convert to CoreML format\ncoreml_model = ct.convert(wav2lip_model, inputs=[\n    ct.TensorType(name=\"face_input\", shape=example_face_input.shape),\n    ct.TensorType(name=\"audio_input\", shape=example_audio_input.shape)\n], source='pytorch')\n\n# Save the CoreML model to a file\ncoreml_model.save(\"converted_wav2lip_model.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:51:47.175829Z","iopub.execute_input":"2024-04-18T18:51:47.176718Z","iopub.status.idle":"2024-04-18T18:51:47.950391Z","shell.execute_reply.started":"2024-04-18T18:51:47.176681Z","shell.execute_reply":"2024-04-18T18:51:47.949158Z"},"trusted":true},"execution_count":48,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m example_audio_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m40\u001b[39m)  \u001b[38;5;66;03m# Example audio input shape: [batch_size, num_channels, time_steps, features]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert to CoreML format\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m coreml_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav2lip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mface_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_face_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_audio_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save the CoreML model to a file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m coreml_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted_wav2lip_model.mlmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/coremltools/converters/_converters_entry.py:533\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    531\u001b[0m source_dialect \u001b[38;5;241m=\u001b[39m _determine_source_dialect(model, exact_source)\n\u001b[1;32m    532\u001b[0m exact_target \u001b[38;5;241m=\u001b[39m _determine_target(convert_to, minimum_deployment_target)\n\u001b[0;32m--> 533\u001b[0m \u001b[43m_validate_conversion_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_deployment_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m need_fp16_cast_pass \u001b[38;5;241m=\u001b[39m _need_fp16_cast_pass(compute_precision, exact_target)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pass_pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/coremltools/converters/_converters_entry.py:877\u001b[0m, in \u001b[0;36m_validate_conversion_arguments\u001b[0;34m(model, exact_source, exact_target, inputs, outputs, classifier_config, compute_precision, convert_to, minimum_deployment_target)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    874\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be a list/tuple (or nested lists/tuples) of TensorType or ImageType\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    875\u001b[0m                 )\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@model must either be a TorchScript object (or .pt or .pth file) or an ExportedProgram object (if using torch.export based API), received: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    879\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(model)\n\u001b[1;32m    880\u001b[0m                 )\n\u001b[1;32m    881\u001b[0m             )\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m exact_source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Program):\n","\u001b[0;31mTypeError\u001b[0m: @model must either be a TorchScript object (or .pt or .pth file) or an ExportedProgram object (if using torch.export based API), received: <class 'models.wav2lip.Wav2Lip'>"],"ename":"TypeError","evalue":"@model must either be a TorchScript object (or .pt or .pth file) or an ExportedProgram object (if using torch.export based API), received: <class 'models.wav2lip.Wav2Lip'>","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip  # Import the Wav2Lip model definition from the cloned repository\n\n# Load the Wav2Lip model checkpoint\ncheckpoint_path = \"checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()\n\n# Convert the PyTorch model to TorchScript\ntorchscript_model = torch.jit.trace(wav2lip_model, (torch.randn(1, 6, 96, 96), torch.randn(1, 1, 100, 40)))\n\n# Define example input shapes for face and audio (adjust according to your model's input shape)\nexample_face_input = torch.randn(1, 6, 96, 96)  # Example face input shape: [batch_size, num_channels, height, width]\nexample_audio_input = torch.randn(1, 1, 100, 40)  # Example audio input shape: [batch_size, num_channels, time_steps, features]\n\n# Convert to CoreML format\ncoreml_model = ct.convert(torchscript_model, inputs=[\n    ct.TensorType(name=\"face_input\", shape=example_face_input.shape),\n    ct.TensorType(name=\"audio_input\", shape=example_audio_input.shape)\n], source='pytorch')\n\n# Save the CoreML model to a file\ncoreml_model.save(\"converted_wav2lip_model.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:53:47.985672Z","iopub.execute_input":"2024-04-18T18:53:47.986693Z","iopub.status.idle":"2024-04-18T18:53:50.362247Z","shell.execute_reply.started":"2024-04-18T18:53:47.986655Z","shell.execute_reply":"2024-04-18T18:53:50.360550Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m wav2lip_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the PyTorch model to TorchScript\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m torchscript_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav2lip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Define example input shapes for face and audio (adjust according to your model's input shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m example_face_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m)  \u001b[38;5;66;03m# Example face input shape: [batch_size, num_channels, height, width]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:798\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m ):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:1065\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1065\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/wav2lip.py:96\u001b[0m, in \u001b[0;36mWav2Lip.forward\u001b[0;34m(self, audio_sequences, face_sequences)\u001b[0m\n\u001b[1;32m     93\u001b[0m     audio_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([audio_sequences[:, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(audio_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m     face_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([face_sequences[:, :, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(face_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m audio_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_sequences\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, 512, 1, 1\u001b[39;00m\n\u001b[1;32m     98\u001b[0m feats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m face_sequences\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/conv.py:16\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m     18\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 6, 96, 96] to have 1 channels, but got 6 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 6, 96, 96] to have 1 channels, but got 6 channels instead","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip  # Import the Wav2Lip model definition from the cloned repository\n\n# Load the Wav2Lip model checkpoint\ncheckpoint_path = \"checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()\n\n# Convert the PyTorch model to TorchScript\ntorchscript_model = torch.jit.trace(wav2lip_model, (torch.randn(1, 6, 96, 96), torch.randn(1, 1, 100, 40)))\n\n# Define example input shapes for face and audio (adjust according to your model's input shape)\nexample_face_input = torch.randn(1, 6, 96, 96)  # Example face input shape: [batch_size, num_channels, height, width]\nexample_audio_input = torch.randn(1, 1, 100, 40)  # Example audio input shape: [batch_size, num_channels, time_steps, features]\n\n# Convert to CoreML format\ncoreml_model = ct.convert(torchscript_model, inputs=[\n    ct.TensorType(name=\"face_input\", shape=example_face_input.shape),\n    ct.TensorType(name=\"audio_input\", shape=example_audio_input.shape)\n], source='pytorch')\n\n# Save the CoreML model to a file\ncoreml_model.save(\"converted_wav2lip_model.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:02:12.815082Z","iopub.execute_input":"2024-04-18T19:02:12.815819Z","iopub.status.idle":"2024-04-18T19:02:14.411699Z","shell.execute_reply.started":"2024-04-18T19:02:12.815771Z","shell.execute_reply":"2024-04-18T19:02:14.410070Z"},"trusted":true},"execution_count":50,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m wav2lip_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the PyTorch model to TorchScript\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m torchscript_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav2lip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Define example input shapes for face and audio (adjust according to your model's input shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m example_face_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m)  \u001b[38;5;66;03m# Example face input shape: [batch_size, num_channels, height, width]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:798\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m ):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:1065\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1065\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/wav2lip.py:96\u001b[0m, in \u001b[0;36mWav2Lip.forward\u001b[0;34m(self, audio_sequences, face_sequences)\u001b[0m\n\u001b[1;32m     93\u001b[0m     audio_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([audio_sequences[:, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(audio_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m     face_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([face_sequences[:, :, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(face_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m audio_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_sequences\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, 512, 1, 1\u001b[39;00m\n\u001b[1;32m     98\u001b[0m feats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m face_sequences\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/conv.py:16\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m     18\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 6, 96, 96] to have 1 channels, but got 6 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 6, 96, 96] to have 1 channels, but got 6 channels instead","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip  # Import the Wav2Lip model definition from the cloned repository\n\n# Load the Wav2Lip model checkpoint\ncheckpoint_path = \"checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()\n\n# Define a new model with the correct input channels for the audio_encoder\nclass FixedAudioEncoder(torch.nn.Module):\n    def __init__(self):\n        super(FixedAudioEncoder, self).__init__()\n        self.audio_encoder = wav2lip_model.audio_encoder\n        self.audio_encoder[0] = torch.nn.Conv2d(6, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n    def forward(self, x):\n        return self.audio_encoder(x)\n\n# Convert the PyTorch model to TorchScript\ntorchscript_model = torch.jit.trace(FixedAudioEncoder(), (torch.randn(1, 6, 96, 96), torch.randn(1, 1, 100, 40)))\n\n# Define example input shapes for face and audio (adjust according to your model's input shape)\nexample_face_input = torch.randn(1, 6, 96, 96)  # Example face input shape: [batch_size, num_channels, height, width]\nexample_audio_input = torch.randn(1, 1, 100, 40)  # Example audio input shape: [batch_size, num_channels, time_steps, features]\n\n# Convert to CoreML format\ncoreml_model = ct.convert(torchscript_model, inputs=[\n    ct.TensorType(name=\"face_input\", shape=example_face_input.shape),\n    ct.TensorType(name=\"audio_input\", shape=example_audio_input.shape)\n], source='pytorch')\n\n# Save the CoreML model to a file\ncoreml_model.save(\"converted_wav2lip_model.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:02:55.494951Z","iopub.execute_input":"2024-04-18T19:02:55.495347Z","iopub.status.idle":"2024-04-18T19:02:56.401877Z","shell.execute_reply.started":"2024-04-18T19:02:55.495309Z","shell.execute_reply":"2024-04-18T19:02:56.400416Z"},"trusted":true},"execution_count":51,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_encoder(x)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convert the PyTorch model to TorchScript\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m torchscript_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFixedAudioEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define example input shapes for face and audio (adjust according to your model's input shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m example_face_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m)  \u001b[38;5;66;03m# Example face input shape: [batch_size, num_channels, height, width]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:798\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m ):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:1065\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1065\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","\u001b[0;31mTypeError\u001b[0m: FixedAudioEncoder.forward() takes 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"FixedAudioEncoder.forward() takes 2 positional arguments but 3 were given","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\nfrom models.wav2lip import Wav2Lip  # Import the Wav2Lip model definition from the cloned repository\n\n# Load the Wav2Lip model checkpoint\ncheckpoint_path = \"checkpoints/wav2lip_gan.pth\"\nwav2lip_model = Wav2Lip()\ncheckpoint = torch.load(checkpoint_path)\nwav2lip_model.load_state_dict(checkpoint['state_dict'])\nwav2lip_model.eval()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:34:45.615673Z","iopub.execute_input":"2024-04-18T20:34:45.616724Z","iopub.status.idle":"2024-04-18T20:34:46.263504Z","shell.execute_reply.started":"2024-04-18T20:34:45.616686Z","shell.execute_reply":"2024-04-18T20:34:46.262554Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"Wav2Lip(\n  (face_encoder_blocks): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(6, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (1): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (2): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (3): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (3): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (4): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (5): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (6): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n  )\n  (audio_encoder): Sequential(\n    (0): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (1): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (2): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (3): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(3, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (4): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (5): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (6): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (7): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (8): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (9): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(3, 2), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (10): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (11): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (12): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n  )\n  (face_decoder_blocks): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (1): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (2): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (3): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(768, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (4): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (5): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(320, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (6): Sequential(\n      (0): Conv2dTranspose(\n        (conv_block): Sequential(\n          (0): ConvTranspose2d(160, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (1): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n      (2): Conv2d(\n        (conv_block): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (act): ReLU()\n      )\n    )\n  )\n  (output_block): Sequential(\n    (0): Conv2d(\n      (conv_block): Sequential(\n        (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ReLU()\n    )\n    (1): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n    (2): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Set the model in evaluation mode.\n# Trace the model with random data.\n\nexample_audio_input = torch.rand(16, 1,  80, 16)  # Adjust parameters as needed (B, T, 1, 80, 16)\nexample_face_input = torch.rand(16, 3, 224, 224)  # Adjust parameters as needed, replace 3 with appropriate number of channels for your face sequence\ndef wrapper_fn(inputs):\n  audio_sequences = inputs[\"audio_sequences\"]\n  face_sequences = inputs[\"face_sequences\"]\n  return wav2lip_model(audio_sequences, face_sequences)\n\n\ntrace_model = torch.jit.trace(wrapper_fn, {\"audio_sequences\": example_audio_input, \"face_sequences\": example_face_input})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:46:10.390084Z","iopub.execute_input":"2024-04-18T20:46:10.390466Z","iopub.status.idle":"2024-04-18T20:46:10.780849Z","shell.execute_reply.started":"2024-04-18T20:46:10.390437Z","shell.execute_reply":"2024-04-18T20:46:10.779154Z"},"trusted":true},"execution_count":86,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[86], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m   face_sequences \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wav2lip_model(audio_sequences, face_sequences)\n\u001b[0;32m---> 12\u001b[0m trace_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_audio_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mface_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_face_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:866\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    856\u001b[0m     traced \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_create_function_from_trace_with_dict(\n\u001b[1;32m    857\u001b[0m         name,\n\u001b[1;32m    858\u001b[0m         func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    863\u001b[0m         get_callable_argument_names(func),\n\u001b[1;32m    864\u001b[0m     )\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m     traced \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_function_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_callable_argument_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_trace:\n","Cell \u001b[0;32mIn[86], line 9\u001b[0m, in \u001b[0;36mwrapper_fn\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m audio_sequences \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m face_sequences \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwav2lip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_sequences\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/wav2lip.py:96\u001b[0m, in \u001b[0;36mWav2Lip.forward\u001b[0;34m(self, audio_sequences, face_sequences)\u001b[0m\n\u001b[1;32m     93\u001b[0m     audio_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([audio_sequences[:, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(audio_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m     face_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([face_sequences[:, :, i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(face_sequences\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m audio_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_sequences\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, 512, 1, 1\u001b[39;00m\n\u001b[1;32m     98\u001b[0m feats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m face_sequences\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/kaggle/working/Wav2Lip/models/conv.py:16\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m     18\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\nTensor:\n(1,1,.,.) = \n -0.2849  0.0494  0.1350\n -0.1719 -0.0786  0.3059\n -0.2688 -0.1040 -0.1412\n\n(2,1,.,.) = \n  0.0833 -0.0769  0.0045\n  0.1553 -0.1660 -0.2939\n  0.3756  0.0214 -0.0273\n\n(3,1,.,.) = \n  0.1263  0.1586  0.0319\n -0.2972  0.0231 -0.2831\n  0.1649 -0.0896  0.2043\n\n(4,1,.,.) = \n -0.2625  0.2454  0.0833\n  0.3284  0.0173  0.1407\n  0.2964 -0.1439 -0.3059\n\n(5,1,.,.) = \n -0.2444  0.3121  0.0638\n  0.0205  0.0466  0.0344\n  0.0894 -0.2584 -0.0880\n\n(6,1,.,.) = \n -0.3399  0.3618  0.0560\n -0.0343  0.0467  0.2836\n  0.1662  0.1220 -0.0975\n\n(7,1,.,.) = \n  0.3211  0.0611 -0.1074\n  0.3291  0.2271 -0.2872\n  0.1964  0.1208  0.2410\n\n(8,1,.,.) = \n -0.0673  0.0224 -0.3329\n  0.2593 -0.2240  0.3489\n -0.2244  0.1752  0.1792\n\n(9,1,.,.) = \n -0.0195 -0.2636  0.1705\n -0.2485 -0.0753  0.1470\n  0.2069  0.1935  0.1350\n\n(10,1,.,.) = \n  0.0277 -0.3266  0.2836\n  0.2555 -0.0924  0.3044\n -0.0215  0.0128 -0.3055\n\n(11,1,.,.) = \n -0.1421  0.3573  0.1551\n -0.0064 -0.1837 -0.0043\n -0.0075 -0.2038 -0.0954\n\n(12,1,.,.) = \n  0.1576  0.2724 -0.2466\n -0.2031  0.3189 -0.2337\n  0.0005 -0.0293  0.1977\n\n(13,1,.,.) = \n  0.1997 -0.1819  0.0354\n -0.2503  0.0650  0.3838\n -0.2805  0.3555 -0.2611\n\n(14,1,.,.) = \n -0.0901 -0.2149  0.1347\n -0.0054 -0.3018 -0.2811\n -0.1563 -0.0974  0.1974\n\n(15,1,.,.) = \n -0.0457 -0.0266 -0.3267\n  0.0327  0.1611  0.3509\n -0.2634 -0.2615  0.2486\n\n(16,1,.,.) = \n  0.3424 -0.0477 -0.2168\n  0.3080 -0.0064 -0.0506\n -0.1473 -0.1706 -0.0963\n\n(17,1,.,.) = \n  0.2850 -0.0511 -0.2159\n -0.1547 -0.0721  0.2407\n -0.0228 -0.0649  0.0787\n\n(18,1,.,.) = \n -0.0988  0.0618 -0.2121\n -0.2173 -0.2498  0.1344\n -0.3672 -0.1525  0.0710\n\n(19,1,.,.) = \n -0.0921  0.1980 -0.2857\n  0.1227  0.2569 -0.2657\n -0.0266  0.1016  0.0310\n\n(20,1,.,.) = \n -0.0219  0.0017  0.2221\n  0.1637  0.2949 -0.1231\n -0.1397 -0.2286 -0.1036\n\n(21,1,.,.) = \n  0.1868 -0.2839 -0.1238\n -0.1917 -0.3410  0.2778\n -0.0039  0.1756 -0.1072\n\n(22,1,.,.) = \n -0.1192  0.0576  0.2964\n  0.0853 -0.2826 -0.2292\n  0.2484 -0.0447  0.1444\n\n(23,1,.,.) = \n -0.2743  0.0993  0.1740\n -0.1291  0.1637 -0.1940\n  0.2016  0.0906 -0.1624\n\n(24,1,.,.) = \n -0.1450  0.2674  0.2532\n -0.0734  0.1401  0.0449\n  0.0763  0.0858  0.0351\n\n(25,1,.,.) = \n  0.0946  0.3474  0.0469\n  0.2296  0.0645  0.2560\n  0.1764 -0.1430 -0.2852\n\n(26,1,.,.) = \n  0.2739  0.2612  0.1166\n -0.1493 -0.2555 -0.0334\n -0.0275 -0.0515 -0.1298\n\n(27,1,.,.) = \n  0.1236 -0.2213 -0.3028\n -0.1496  0.1639  0.1085\n  0.0094  0.1025  0.1916\n\n(28,1,.,.) = \n  0.2775  0.3092  0.2545\n -0.0932  0.0014  0.2236\n -0.2240 -0.3402  0.2044\n\n(29,1,.,.) = \n -0.0491  0.1747 -0.0440\n  0.2268 -0.2758 -0.1798\n  0.0849 -0.0054 -0.1528\n\n(30,1,.,.) = \n  0.0593  0.3520  0.0552\n  0.2907 -0.2408 -0.0040\n -0.3094 -0.2979  0.1228\n\n(31,1,.,.) = \n -0.0011  0.3318 -0.2145\n -0.3360  0.1518  0.3259\n  0.2134 -0.1470 -0.2856\n\n(32,1,.,.) = \n  0.3212  0.0190  0.1875\n -0.1264 -0.2413 -0.2956\n -0.0581 -0.3145  0.1649\n[ torch.FloatTensor{32,1,3,3} ]"],"ename":"RuntimeError","evalue":"Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\nTensor:\n(1,1,.,.) = \n -0.2849  0.0494  0.1350\n -0.1719 -0.0786  0.3059\n -0.2688 -0.1040 -0.1412\n\n(2,1,.,.) = \n  0.0833 -0.0769  0.0045\n  0.1553 -0.1660 -0.2939\n  0.3756  0.0214 -0.0273\n\n(3,1,.,.) = \n  0.1263  0.1586  0.0319\n -0.2972  0.0231 -0.2831\n  0.1649 -0.0896  0.2043\n\n(4,1,.,.) = \n -0.2625  0.2454  0.0833\n  0.3284  0.0173  0.1407\n  0.2964 -0.1439 -0.3059\n\n(5,1,.,.) = \n -0.2444  0.3121  0.0638\n  0.0205  0.0466  0.0344\n  0.0894 -0.2584 -0.0880\n\n(6,1,.,.) = \n -0.3399  0.3618  0.0560\n -0.0343  0.0467  0.2836\n  0.1662  0.1220 -0.0975\n\n(7,1,.,.) = \n  0.3211  0.0611 -0.1074\n  0.3291  0.2271 -0.2872\n  0.1964  0.1208  0.2410\n\n(8,1,.,.) = \n -0.0673  0.0224 -0.3329\n  0.2593 -0.2240  0.3489\n -0.2244  0.1752  0.1792\n\n(9,1,.,.) = \n -0.0195 -0.2636  0.1705\n -0.2485 -0.0753  0.1470\n  0.2069  0.1935  0.1350\n\n(10,1,.,.) = \n  0.0277 -0.3266  0.2836\n  0.2555 -0.0924  0.3044\n -0.0215  0.0128 -0.3055\n\n(11,1,.,.) = \n -0.1421  0.3573  0.1551\n -0.0064 -0.1837 -0.0043\n -0.0075 -0.2038 -0.0954\n\n(12,1,.,.) = \n  0.1576  0.2724 -0.2466\n -0.2031  0.3189 -0.2337\n  0.0005 -0.0293  0.1977\n\n(13,1,.,.) = \n  0.1997 -0.1819  0.0354\n -0.2503  0.0650  0.3838\n -0.2805  0.3555 -0.2611\n\n(14,1,.,.) = \n -0.0901 -0.2149  0.1347\n -0.0054 -0.3018 -0.2811\n -0.1563 -0.0974  0.1974\n\n(15,1,.,.) = \n -0.0457 -0.0266 -0.3267\n  0.0327  0.1611  0.3509\n -0.2634 -0.2615  0.2486\n\n(16,1,.,.) = \n  0.3424 -0.0477 -0.2168\n  0.3080 -0.0064 -0.0506\n -0.1473 -0.1706 -0.0963\n\n(17,1,.,.) = \n  0.2850 -0.0511 -0.2159\n -0.1547 -0.0721  0.2407\n -0.0228 -0.0649  0.0787\n\n(18,1,.,.) = \n -0.0988  0.0618 -0.2121\n -0.2173 -0.2498  0.1344\n -0.3672 -0.1525  0.0710\n\n(19,1,.,.) = \n -0.0921  0.1980 -0.2857\n  0.1227  0.2569 -0.2657\n -0.0266  0.1016  0.0310\n\n(20,1,.,.) = \n -0.0219  0.0017  0.2221\n  0.1637  0.2949 -0.1231\n -0.1397 -0.2286 -0.1036\n\n(21,1,.,.) = \n  0.1868 -0.2839 -0.1238\n -0.1917 -0.3410  0.2778\n -0.0039  0.1756 -0.1072\n\n(22,1,.,.) = \n -0.1192  0.0576  0.2964\n  0.0853 -0.2826 -0.2292\n  0.2484 -0.0447  0.1444\n\n(23,1,.,.) = \n -0.2743  0.0993  0.1740\n -0.1291  0.1637 -0.1940\n  0.2016  0.0906 -0.1624\n\n(24,1,.,.) = \n -0.1450  0.2674  0.2532\n -0.0734  0.1401  0.0449\n  0.0763  0.0858  0.0351\n\n(25,1,.,.) = \n  0.0946  0.3474  0.0469\n  0.2296  0.0645  0.2560\n  0.1764 -0.1430 -0.2852\n\n(26,1,.,.) = \n  0.2739  0.2612  0.1166\n -0.1493 -0.2555 -0.0334\n -0.0275 -0.0515 -0.1298\n\n(27,1,.,.) = \n  0.1236 -0.2213 -0.3028\n -0.1496  0.1639  0.1085\n  0.0094  0.1025  0.1916\n\n(28,1,.,.) = \n  0.2775  0.3092  0.2545\n -0.0932  0.0014  0.2236\n -0.2240 -0.3402  0.2044\n\n(29,1,.,.) = \n -0.0491  0.1747 -0.0440\n  0.2268 -0.2758 -0.1798\n  0.0849 -0.0054 -0.1528\n\n(30,1,.,.) = \n  0.0593  0.3520  0.0552\n  0.2907 -0.2408 -0.0040\n -0.3094 -0.2979  0.1228\n\n(31,1,.,.) = \n -0.0011  0.3318 -0.2145\n -0.3360  0.1518  0.3259\n  0.2134 -0.1470 -0.2856\n\n(32,1,.,.) = \n  0.3212  0.0190  0.1875\n -0.1264 -0.2413 -0.2956\n -0.0581 -0.3145  0.1649\n[ torch.FloatTensor{32,1,3,3} ]","output_type":"error"}]},{"cell_type":"code","source":"os.remove(\"/kaggle/working/Wav2Lip/models/wav2lip.py\")\n!cp \"/kaggle/input/modelfile/wav2lip.py\" /kaggle/working/Wav2Lip/models\n!ls /kaggle/working/Wav2Lip/models","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:23:17.856747Z","iopub.execute_input":"2024-04-18T20:23:17.857690Z","iopub.status.idle":"2024-04-18T20:23:19.808783Z","shell.execute_reply.started":"2024-04-18T20:23:17.857652Z","shell.execute_reply":"2024-04-18T20:23:19.807654Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"__init__.py  __pycache__  conv.py  syncnet.py  wav2lip.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport coremltools as ct\n\n# Instantiate the model\nmodel = Wav2Lip()\n\n# Load the state dict if needed\n# model.load_state_dict(model_state_dict)\n\n# Convert the model to TorchScript if not already\nif not isinstance(model, torch.jit.ScriptModule):\n    model = torch.jit.script(model)\n\n# Define input and output shapes\ninput_shape = (batch_size, sequence_length, num_channels_audio, height_audio, width_audio)\noutput_shape = (batch_size, num_channels_face, sequence_length, height_face, width_face)\n\n# Define input and output names\ninput_name = \"audio_sequences\"\noutput_name = \"output\"\n\n# Define placeholder values for input and output shapes\nbatch_size = 1\nsequence_length = 10\nnum_channels_audio = 1\nheight_audio = 80\nwidth_audio = 16\nnum_channels_face = 3\nheight_face = 96\nwidth_face = 96\n\n# Create a dummy input tensor with placeholder values\naudio_sequences = torch.randn(input_shape)\n\n# Define the CoreML model\ncoreml_model = ct.convert(\n    model,\n    inputs=[ct.TensorType(shape=input_shape, name=input_name)],\n    outputs=[ct.TensorType(shape=output_shape, name=output_name)]\n)\n\n# Save the CoreML model\ncoreml_model.save(\"wav2lip.mlmodel\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:31:12.282440Z","iopub.execute_input":"2024-04-18T20:31:12.283182Z","iopub.status.idle":"2024-04-18T20:31:13.665693Z","shell.execute_reply.started":"2024-04-18T20:31:12.283150Z","shell.execute_reply":"2024-04-18T20:31:13.664382Z"},"trusted":true},"execution_count":71,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[71], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the state dict if needed\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(model_state_dict)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert the model to TorchScript if not already\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule):\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Define input and output shapes\u001b[39;00m\n\u001b[1;32m     15\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (batch_size, sequence_length, num_channels_audio, height_audio, width_audio)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:1324\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1323\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:559\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    558\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:636\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 636\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:469\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    466\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    467\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 469\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Can't redefine method: forward on class: __torch__.models.wav2lip.___torch_mangle_1702.Wav2Lip (of Python compilation unit at: 0x569d661453f0)"],"ename":"RuntimeError","evalue":"Can't redefine method: forward on class: __torch__.models.wav2lip.___torch_mangle_1702.Wav2Lip (of Python compilation unit at: 0x569d661453f0)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}